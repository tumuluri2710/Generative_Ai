{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518c093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lalitha\\OneDrive\\Desktop\\LLM\\Generative_Ai\\LLM_ENV\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e5e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not huggingface_token:\n",
    "    print(\"HUGGINGFACE_TOKEN not found in .env file. Please add it.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed5c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Log in to Hugging Face\n",
    "try:\n",
    "    login(token=huggingface_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging in to Hugging Face: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba1bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "loader = TextLoader(\"sample_data.txt\")  # Replace with your data file\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371889b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split Text into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde4d3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lalitha\\AppData\\Local\\Temp\\ipykernel_5328\\335209700.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a48ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Vector Database\n",
    "vectordb = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8d4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3dde968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_with_context(question: str):\n",
    "    \"\"\"\n",
    "    Answers a question using a Hugging Face LLM with context from a vector database.\n",
    "    \"\"\"\n",
    "    # Search the vector database for relevant chunks\n",
    "    relevant_chunks = vectordb.similarity_search(question, k=3)  # Retrieve top 3 relevant chunks\n",
    "\n",
    "    # Extract the text content of the relevant chunks\n",
    "    context = \" \".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "    # Prepare the input for the question-answering pipeline\n",
    "    qa_input = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "    # Get the answer from the pipeline\n",
    "    result = question_answerer(qa_input)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"\\nContext:\\n{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e92304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lalitha\\OneDrive\\Desktop\\LLM\\Generative_Ai\\LLM_ENV\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was the Eiffel Tower constructed?\n",
      "Answer: 1887 to 1889\n",
      "\n",
      "Context:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
      "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
      "Constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
      "The tower is 330 metres (1,083 ft) tall, about the same height as an 81-story building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. Due to the addition of a broadcasting aerial at the top in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding antennae, the Eiffel Tower is the second tallest structure in France after the Millau Viaduct.\n",
      "The tower has three levels for visitors, with restaurants on the first and second levels. The top-level's upper platform is 276 m (906 ft) above the ground â€“ the highest observation deck accessible to the public in the European Union and the second highest in Europe after the Ostankino Tower in Moscow.\n",
      "\n",
      "---\n",
      "Question: Who was the Eiffel Tower named after?\n",
      "Answer: Gustave Eiffel\n",
      "\n",
      "Context:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
      "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
      "Constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
      "The tower is 330 metres (1,083 ft) tall, about the same height as an 81-story building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. Due to the addition of a broadcasting aerial at the top in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding antennae, the Eiffel Tower is the second tallest structure in France after the Millau Viaduct.\n",
      "The tower has three levels for visitors, with restaurants on the first and second levels. The top-level's upper platform is 276 m (906 ft) above the ground â€“ the highest observation deck accessible to the public in the European Union and the second highest in Europe after the Ostankino Tower in Moscow.\n",
      "\n",
      "---\n",
      "Question: How tall is the Eiffel Tower?\n",
      "Answer: 330 metres (1,083 ft)\n",
      "\n",
      "Context:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
      "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
      "Constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
      "The tower is 330 metres (1,083 ft) tall, about the same height as an 81-story building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. Due to the addition of a broadcasting aerial at the top in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding antennae, the Eiffel Tower is the second tallest structure in France after the Millau Viaduct.\n",
      "The tower has three levels for visitors, with restaurants on the first and second levels. The top-level's upper platform is 276 m (906 ft) above the ground â€“ the highest observation deck accessible to the public in the European Union and the second highest in Europe after the Ostankino Tower in Moscow.\n"
     ]
    }
   ],
   "source": [
    "ask_llm_with_context(\"When was the Eiffel Tower constructed?\")\n",
    "print(\"\\n---\")\n",
    "ask_llm_with_context(\"Who was the Eiffel Tower named after?\")\n",
    "print(\"\\n---\")\n",
    "ask_llm_with_context(\"How tall is the Eiffel Tower?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
